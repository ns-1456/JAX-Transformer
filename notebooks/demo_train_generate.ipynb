{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CS336 Assignment 1 (basics): Building a Transformer LM\n",
        "\n",
        "This notebook follows **Stanford CS336 Spring 2025 Assignment 1** (cs336_spring2025_assignment1_basics.pdf). It uses this repo's JAX implementation (BPE tokenizer, Transformer LM, AdamW, training, generation).\n",
        "\n",
        "**What you will implement (per PDF):** §2 BPE tokenizer, §3 Transformer LM, §4 cross-entropy & AdamW, §5 training loop & checkpointing, §6 decoding.\n",
        "\n",
        "**End-to-end (no manual steps):** This notebook downloads TinyStories only, trains BPE on it, runs tokenizer experiments, trains the Transformer LM, generates ≥256 tokens, and runs LR + batch-size experiments. Run all cells in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup (Colab: clone + install; Local: set cwd or LM_PROJECT_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    repo = Path(\"/content/JAX-Transformer\")\n",
        "    if not (repo / \"src\" / \"tokenizer.py\").exists():\n",
        "        get_ipython().system(\"git clone --depth 1 https://github.com/ns-1456/JAX-Transformer.git /content/JAX-Transformer\")\n",
        "    ROOT = repo\n",
        "    get_ipython().run_line_magic(\"pip\", \"install -q -r \" + str(ROOT / \"requirements.txt\"))\n",
        "else:\n",
        "    marker = Path(\"src\") / \"tokenizer.py\"\n",
        "    ROOT = Path(os.environ.get(\"LM_PROJECT_ROOT\", \"\")).resolve() if os.environ.get(\"LM_PROJECT_ROOT\") else None\n",
        "    if not ROOT or not (ROOT / marker).exists():\n",
        "        cwd = Path.cwd().resolve()\n",
        "        for _ in range(8):\n",
        "            if (cwd / marker).exists(): ROOT = cwd; break\n",
        "            if (cwd / \"project-2-lm-jax\" / marker).exists(): ROOT = cwd / \"project-2-lm-jax\"; break\n",
        "            if cwd == cwd.parent: break\n",
        "            cwd = cwd.parent\n",
        "    if not ROOT or not (ROOT / marker).exists():\n",
        "        raise FileNotFoundError(\"Set LM_PROJECT_ROOT or run from project-2-lm-jax.\")\n",
        "sys.path.insert(0, str(ROOT / \"src\"))\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tokenizer import train_bpe, Tokenizer\n",
        "from model import TransformerConfig, TransformerLM\n",
        "from optimizer import init_adamw, adamw_step\n",
        "from train import get_batch, loss_fn, save_checkpoint\n",
        "from generate import generate\n",
        "\n",
        "DEMO_DIR = ROOT / \"notebooks\" / \"demo_outputs\"\n",
        "DATA_DIR = DEMO_DIR / \"data\"\n",
        "DEMO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"JAX devices:\", jax.devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download datasets (TinyStories only)\n",
        "\n",
        "Uses the same TinyStories data as [stanford-cs336/assignment1-basics](https://github.com/stanford-cs336/assignment1-basics): TinyStoriesV2-GPT4 train + valid. No manual wget or file placement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "EOT = \"<|endoftext|>\"\n",
        "\n",
        "# TinyStories (same as assignment: roneneldan/TinyStories, TinyStoriesV2-GPT4-train/valid.txt)\n",
        "print(\"Downloading TinyStoriesV2-GPT4 (train + valid)...\")\n",
        "for split in (\"train\", \"valid\"):\n",
        "    fn = f\"TinyStoriesV2-GPT4-{split}.txt\"\n",
        "    cached = hf_hub_download(repo_id=\"roneneldan/TinyStories\", filename=fn, repo_type=\"dataset\")\n",
        "    dest = DATA_DIR / fn\n",
        "    shutil.copy(cached, dest)\n",
        "    print(f\"  {fn} -> {dest}\")\n",
        "path_ts = DATA_DIR / \"TinyStoriesV2-GPT4-train.txt\"\n",
        "\n",
        "print(\"Done. Using path_ts for BPE and training below.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## §2 Byte-Pair Encoding (BPE) Tokenizer\n",
        "\n",
        "### §2.1 The Unicode Standard\n",
        "\n",
        "**Problem (unicode1):** (a) What character does `chr(0)` return? (b) How does `__repr__` differ from printed representation? (c) What happens when it occurs in text?\n",
        "\n",
        "**Deliverable:** One-sentence each in writeup. Optional exploration below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: explore chr(0) for writeup\n",
        "c = chr(0)\n",
        "print(\"repr:\", repr(c)); print(\"print:\", repr(print(c)))\n",
        "s = \"this is a test\" + chr(0) + \"string\"\n",
        "print(\"repr(s):\", repr(s)); print(\"print(s):\"); print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### §2.2 Unicode Encodings\n",
        "\n",
        "**Problem (unicode2):** (a) Why prefer UTF-8 over UTF-16/32? (b) Why is `decode_utf8_bytes_to_str_wrong` incorrect? Example? (c) Two-byte sequence that does not decode.\n",
        "\n",
        "**Deliverable:** Short answers in writeup."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### §2.4 BPE Tokenizer Training\n",
        "\n",
        "**Problem (train_bpe):** Train byte-level BPE: `input_path`, `vocab_size`, `special_tokens` → `(vocab, merges)`. GPT-2 regex pre-tokenization; lexicographic tie-break; strip special tokens before pre-tokenization.\n",
        "\n",
        "**Implementation:** `src/tokenizer.py` → `train_bpe()`. Merges here are `list[tuple[int,int]]` (IDs); PDF uses `tuple[bytes,bytes]`—convert via vocab if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLE = (\n",
        "    \"The little dog ran in the park. The cat sat on the mat. \"\n",
        "    \"Once upon a time there was a girl. She had a big red ball. \"\n",
        ") * 80\n",
        "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".txt\", delete=False) as f:\n",
        "    f.write(SAMPLE)\n",
        "    _path = f.name\n",
        "try:\n",
        "    vocab, merges = train_bpe(_path, vocab_size=1024, special_tokens=[\"<|endoftext|>\"])\n",
        "finally:\n",
        "    os.unlink(_path)\n",
        "tokenizer = Tokenizer(vocab=vocab, merges=merges, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.save(str(DEMO_DIR / \"tokenizer.json\"))\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Encode 'The dog ran':\", tokenizer.encode(\"The dog ran.\")[:8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### §2.5 BPE on TinyStories\n",
        "\n",
        "**Problem (train_bpe_tinystories):** Train BPE on TinyStories, vocab_size=10000, special token `<|endoftext|>`. Report: time, memory, longest token.\n",
        "\n",
        "**Deliverable:** Run with dataset path; report in writeup. Example: `train_bpe(path, 10000, [\"<|endoftext|>\"])` then inspect vocab/merges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# §2.5: Train BPE on downloaded TinyStories only.\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "path_ts = DATA_DIR / \"TinyStoriesV2-GPT4-train.txt\"\n",
        "if not path_ts.exists():\n",
        "    raise FileNotFoundError(\"Run the 'Download datasets' cell first. It writes to demo_outputs/data/.\")\n",
        "EOT = \"<|endoftext|>\"  # in case this cell is run without the download cell\n",
        "\n",
        "def run_bpe_section(name, path, vocab_size, special_tokens, out_json):\n",
        "    \"\"\"Train BPE, report time/memory/longest token, save tokenizer.\"\"\"\n",
        "    path = Path(path)\n",
        "    print(f\"--- {name} (vocab_size={vocab_size}) ---\")\n",
        "    tracemalloc.start()\n",
        "    t0 = time.perf_counter()\n",
        "    # use_fast=True uses Hugging Face tokenizers (Rust); max_bytes=5M for ~1-2 s demo.\n",
        "    vocab, merges = train_bpe(str(path), vocab_size=vocab_size, special_tokens=special_tokens, max_bytes=5_000_000, use_fast=True)\n",
        "    elapsed = time.perf_counter() - t0\n",
        "    _, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    tok = Tokenizer(vocab=vocab, merges=merges, special_tokens=special_tokens)\n",
        "    tok.save(str(out_json))\n",
        "    longest = max((len(v) for v in vocab.values()), default=0)\n",
        "    longest_tok = next((v for v in vocab.values() if len(v) == longest), b\"\")\n",
        "    print(f\"  Time: {elapsed:.1f}s  |  Peak RAM: {peak / 1024**3:.2f} GB  |  Longest token: {len(longest_tok)} bytes (e.g. {longest_tok[:50]!r})\")\n",
        "    return tok\n",
        "\n",
        "tok_ts = run_bpe_section(\"TinyStories\", path_ts, 10000, [EOT], DEMO_DIR / \"tokenizer_tinystories.json\")\n",
        "tokenizer = tok_ts\n",
        "vocab_size = len(tok_ts.vocab)\n",
        "print(\"Using TinyStories tokenizer for LM.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### §2.6 BPE Tokenizer: Encoding and Decoding\n",
        "\n",
        "**Problem (tokenizer):** Implement Tokenizer: `encode`, `decode`, `encode_iterable` (for large files), `save`/`load`. Decode uses `errors='replace'` for malformed bytes.\n",
        "\n",
        "**Implementation:** `src/tokenizer.py` → `Tokenizer` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"The cat sat on the mat.\"\n",
        "ids = tokenizer.encode(text)\n",
        "decoded = tokenizer.decode(ids)\n",
        "print(\"ids:\", ids)\n",
        "print(\"decode:\", decoded)\n",
        "print(\"roundtrip ok:\", decoded == text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### §2.7 Tokenizer Experiments\n",
        "\n",
        "**Problem (tokenizer_experiments):** (a) Sample 10 docs; compression ratio (bytes/token) for TinyStories tokenizer. (b) Throughput (bytes/s); time for 825GB. (c) Encode train to uint16; why uint16?\n",
        "\n",
        "**Deliverable:** Report in writeup. Below: compression ratio and throughput on TinyStories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# §2.7: Tokenizer experiments on TinyStories (10 docs, compression, throughput, 825GB, uint16).\n",
        "with open(path_ts) as f:\n",
        "    full_ts = f.read()\n",
        "docs_ts = full_ts.split(EOT)[:10]\n",
        "\n",
        "def compression_ratio(tok, docs):\n",
        "    bytes_ = sum(len(d.encode(\"utf-8\")) for d in docs)\n",
        "    tokens = sum(len(tok.encode(d)) for d in docs)\n",
        "    return bytes_ / max(1, tokens)\n",
        "\n",
        "cr_ts = compression_ratio(tok_ts, docs_ts)\n",
        "print(f\"(a) Compression ratio (bytes/token) on 10 TinyStories docs: {cr_ts:.2f}\")\n",
        "\n",
        "sample = full_ts[:100000]\n",
        "t0 = time.perf_counter()\n",
        "for _ in range(5):\n",
        "    tok_ts.encode(sample)\n",
        "throughput = (5 * len(sample.encode(\"utf-8\"))) / (time.perf_counter() - t0)\n",
        "print(f\"(b) Throughput: ~{throughput:.0f} bytes/s  ->  time for 825GB: ~{825e9/throughput/3600:.1f} hours\")\n",
        "print(\"(c) uint16: vocab <=65536 so token IDs fit in uint16; saves memory.\")\n",
        "\n",
        "ids_ts = np.array(tok_ts.encode(full_ts), dtype=np.uint16)\n",
        "np.save(DEMO_DIR / \"tinystories_ids.npy\", ids_ts)\n",
        "print(f\"Encoded TinyStories to uint16: {ids_ts.nbytes/1e6:.1f} MB saved to tinystories_ids.npy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## §3 Transformer Language Model\n",
        "\n",
        "**Problems:** linear, embedding, RMSNorm, positionwise_feedforward (SwiGLU), RoPE, softmax, scaled_dot_product_attention, multihead_self_attention, transformer_block, transformer_lm.\n",
        "\n",
        "**Implementation:** `src/model.py` — pre-norm Transformer block, RoPE on Q/K, SwiGLU FFN, causal masking, tied LM head.\n",
        "\n",
        "**Problem (transformer_accounting):** FLOPs, params, memory for GPT-2 XL/small/medium/large; which parts dominate. **Deliverable:** Writeup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size, seq_len = 2, 64\n",
        "cfg = TransformerConfig(vocab_size=vocab_size, d_model=128, num_layers=4, num_heads=4, d_ff=512, max_seq_len=seq_len, dropout_rate=0.1)\n",
        "model = TransformerLM(cfg)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "params = model.init(rng, jnp.ones((batch_size, seq_len), dtype=jnp.int32), deterministic=True)\n",
        "x = jnp.array(tokenizer.encode(\"The dog ran.\")[:seq_len], dtype=jnp.int32)\n",
        "x = jnp.broadcast_to(x, (batch_size, seq_len))\n",
        "logits = model.apply(params, x, deterministic=True)\n",
        "print(\"logits shape:\", logits.shape)\n",
        "nparams = sum(np.size(v) for _, v in jax.tree_util.tree_leaves(params))\n",
        "print(\"Param count:\", nparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## §4 Training a Transformer LM\n",
        "\n",
        "**Problems:** cross_entropy, learning_rate_tuning (SGD toy), adamw, adamwAccounting, learning_rate_schedule (cosine+warmup), gradient_clipping, data_loading, checkpointing.\n",
        "\n",
        "**Implementation:** `src/train.py` (cross_entropy_loss, get_batch, save/load_checkpoint), `src/optimizer.py` (AdamW).\n",
        "\n",
        "**Deliverables (writeup):** learning_rate_tuning (SGD 1e1, 1e2, 1e3), adamwAccounting (peak memory, max batch 80GB, FLOPs/step, MFU, days 400K steps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = np.load(DEMO_DIR / \"tinystories_ids.npy\").astype(np.int32)\n",
        "batch_size, seq_len, num_steps = 16, 128, 800\n",
        "cfg = TransformerConfig(vocab_size=vocab_size, d_model=256, num_layers=4, num_heads=8, d_ff=1024, max_seq_len=seq_len, dropout_rate=0.1)\n",
        "model = TransformerLM(cfg)\n",
        "rng = jax.random.PRNGKey(42)\n",
        "params = model.init(rng, jnp.ones((1, seq_len), dtype=jnp.int32), deterministic=True)\n",
        "opt_state = init_adamw(params)\n",
        "np_rng = np.random.default_rng(42)\n",
        "grad_fn = jax.jit(jax.grad(lambda p, b: loss_fn(p, model, b)))\n",
        "jit_loss = jax.jit(lambda p, b: loss_fn(p, model, b))\n",
        "lr, wd = 3e-4, 0.01\n",
        "losses = []\n",
        "for step in tqdm(range(1, num_steps + 1), desc=\"Training\"):\n",
        "    batch = jnp.array(get_batch(data, batch_size, seq_len, np_rng))\n",
        "    grads = grad_fn(params, batch)\n",
        "    params, opt_state = adamw_step(params, grads, opt_state, lr=lr, weight_decay=wd)\n",
        "    if step % 50 == 0:\n",
        "        losses.append((step, float(jit_loss(params, batch))))\n",
        "save_checkpoint(str(DEMO_DIR / \"ckpt.pkl\"), params, opt_state, num_steps, cfg)\n",
        "print(\"Checkpoint saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "steps, vals = zip(*losses)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(steps, vals, \"b-\", label=\"Loss\")\n",
        "plt.plot(steps, np.exp(vals), \"g-\", alpha=0.7, label=\"Perplexity\")\n",
        "plt.xlabel(\"Step\"); plt.ylabel(\"Loss / Perplexity\"); plt.legend(); plt.title(\"Training on TinyStories\"); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## §5 Training Loop\n",
        "\n",
        "**Problem (training_together):** Script with configurable hyperparameters, memory-efficient data loading (e.g. memmap), checkpointing, logging.\n",
        "\n",
        "**Implementation:** `src/train.py`; CLI: `python src/train.py --data ... --tokenizer ... --config small`. Notebook above is a short in-memory version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## §6 Generating Text\n",
        "\n",
        "**Problem (decoding):** Generate from prompt until `<|endoftext|>` or max_tokens; temperature scaling; top-p (nucleus) sampling.\n",
        "\n",
        "**Implementation:** `src/generate.py` — temperature, top_k; top_p can be added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = generate(model, params, tokenizer, \"Once upon a time\", max_tokens=280, temperature=0.8, top_k=40, seed=99)\n",
        "print(\"Generated (>=256 tokens):\")\n",
        "print(out)\n",
        "print(f\"\\nLength: {len(out)} chars, ~{len(tokenizer.encode(out))} tokens.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## §7 Experiments\n",
        "\n",
        "**experiment_log:** Logging + experiment log document.\n",
        "\n",
        "**learning_rate:** Sweep LR on TinyStories; curves; val loss ≤1.45 (or ≤2.0 low-resource).\n",
        "\n",
        "**batch_size_experiment:** Vary batch size; curves + commentary.\n",
        "\n",
        "**generate:** ≥256 tokens dump + comment on fluency + factors (temperature, top-p, etc.).\n",
        "\n",
        "**Ablations (writeup + curves):** layer_norm_ablation, pre_norm_ablation, no_pos_emb (NoPE vs RoPE), swiglu_ablation (SwiGLU vs SiLU, matched params).\n",
        "\n",
        "**main_experiment:** Train on OpenWebText; learning curve; generated text; compare to TinyStories.\n",
        "\n",
        "**leaderboard (optional):** Submit validation perplexity; see course repo.\n",
        "\n",
        "**Full checklist:** `docs/CS336_ASSIGNMENT1_CHECKLIST.md` maps every A1 problem to this codebase and writeup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# §7 Learning rate sweep (run all experiments; no manual steps)\n",
        "exp_steps = 120\n",
        "lrs = [1e-4, 5e-4, 2e-3]\n",
        "lr_curves = {lr: [] for lr in lrs}\n",
        "for lr in lrs:\n",
        "    p = model.init(jax.random.PRNGKey(0), jnp.ones((1, seq_len), dtype=jnp.int32), deterministic=True)\n",
        "    o = init_adamw(p)\n",
        "    r = np.random.default_rng(int(lr * 1e6))\n",
        "    for step in range(1, exp_steps + 1):\n",
        "        batch = jnp.array(get_batch(data, 8, seq_len, r))\n",
        "        g = grad_fn(p, batch)\n",
        "        p, o = adamw_step(p, g, o, lr=lr, weight_decay=wd)\n",
        "        if step % 30 == 0:\n",
        "            lr_curves[lr].append((step, float(jit_loss(p, batch))))\n",
        "plt.figure(figsize=(8, 4))\n",
        "for lr in lrs:\n",
        "    s, v = zip(*lr_curves[lr])\n",
        "    plt.plot(s, v, label=f\"lr={lr}\")\n",
        "plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Learning rate sweep\"); plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# §7 Batch size experiment\n",
        "batch_sizes = [4, 16, 64]\n",
        "bs_curves = {bs: [] for bs in batch_sizes}\n",
        "for bs in batch_sizes:\n",
        "    p = model.init(jax.random.PRNGKey(1), jnp.ones((1, seq_len), dtype=jnp.int32), deterministic=True)\n",
        "    o = init_adamw(p)\n",
        "    r = np.random.default_rng(42 + bs)\n",
        "    for step in range(1, exp_steps + 1):\n",
        "        batch = jnp.array(get_batch(data, bs, seq_len, r))\n",
        "        g = grad_fn(p, batch)\n",
        "        p, o = adamw_step(p, g, o, lr=5e-4, weight_decay=wd)\n",
        "        if step % 30 == 0:\n",
        "            bs_curves[bs].append((step, float(jit_loss(p, batch))))\n",
        "plt.figure(figsize=(8, 4))\n",
        "for bs in batch_sizes:\n",
        "    s, v = zip(*bs_curves[bs])\n",
        "    plt.plot(s, v, label=f\"batch={bs}\")\n",
        "plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.legend(); plt.title(\"Batch size experiment\"); plt.tight_layout(); plt.show()\n",
        "print(\"Done. All assignment runs completed end-to-end.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
