{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transformer LM Demo: Train & Generate\n",
        "\n",
        "Short training demo and text generation with the JAX/XLA Transformer LM (BPE, RoPE, RMSNorm, SwiGLU, KV cache, MLA/DSA).\n",
        "\n",
        "**Google Colab:** Run the first cell to clone the repo and install dependencies, then run all.  \n",
        "**Local:** Run from `project-2-lm-jax/` (or set kernel cwd there)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "import tempfile\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Colab: clone repo and install deps; local: use cwd ---\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    repo_dir = Path(\"/content/JAX-XLA-LM\")\n",
        "    if not (repo_dir / \"src\" / \"tokenizer.py\").exists():\n",
        "        get_ipython().system(\"git clone --depth 1 https://github.com/ns-1456/JAX-XLA-LM.git /content/JAX-XLA-LM\")\n",
        "    ROOT = repo_dir\n",
        "    get_ipython().run_line_magic(\"pip\", \"install -q -r \" + str(ROOT / \"requirements.txt\"))\n",
        "else:\n",
        "    ROOT = Path.cwd()\n",
        "    if not (ROOT / \"src\" / \"model.py\").exists() and (ROOT.parent / \"src\" / \"model.py\").exists():\n",
        "        ROOT = ROOT.parent\n",
        "\n",
        "sys.path.insert(0, str(ROOT / \"src\"))\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tokenizer import train_bpe, Tokenizer\n",
        "from model import TransformerConfig, TransformerLM, TINY_CONFIG\n",
        "from optimizer import init_adamw, adamw_step\n",
        "from train import cross_entropy_loss, get_batch, loss_fn, save_checkpoint\n",
        "from generate import generate, init_mla_params\n",
        "\n",
        "DEMO_DIR = ROOT / \"notebooks\" / \"demo_outputs\"\n",
        "DEMO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"JAX devices:\", jax.devices())\n",
        "print(\"Colab:\", IN_COLAB)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tokenizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3481216375.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"src\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_bpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransformerConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTINY_CONFIG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_adamw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madamw_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tokenizer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Train BPE tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SAMPLE = (\n",
        "    \"The little dog ran in the park. The cat sat on the mat. \"\n",
        "    \"Once upon a time there was a girl. She had a big red ball. \"\n",
        "    \"The sun was bright. The bird flew high in the sky. \"\n",
        ") * 100\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".txt\", delete=False) as f:\n",
        "    f.write(SAMPLE)\n",
        "    temp_path = f.name\n",
        "try:\n",
        "    vocab, merges = train_bpe(temp_path, vocab_size=1024, special_tokens=[\"<|endoftext|>\"])\n",
        "finally:\n",
        "    os.unlink(temp_path)\n",
        "\n",
        "tokenizer = Tokenizer(vocab=vocab, merges=merges, special_tokens=[\"<|endoftext|>\"])\n",
        "tokenizer.save(str(DEMO_DIR / \"tokenizer.json\"))\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "print(\"Encode 'The dog ran':\", tokenizer.encode(\"The dog ran.\")[:8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare data and train model (short run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = np.array(tokenizer.encode(SAMPLE), dtype=np.int32)\n",
        "batch_size, seq_len, num_steps = 8, 64, 200\n",
        "cfg = TransformerConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=128,\n",
        "    num_layers=4,\n",
        "    num_heads=4,\n",
        "    d_ff=512,\n",
        "    max_seq_len=seq_len,\n",
        "    dropout_rate=0.1,\n",
        ")\n",
        "\n",
        "model = TransformerLM(cfg)\n",
        "rng = jax.random.PRNGKey(42)\n",
        "np_rng = np.random.default_rng(42)\n",
        "params = model.init(rng, jnp.ones((1, seq_len), dtype=jnp.int32), deterministic=True)\n",
        "opt_state = init_adamw(params)\n",
        "\n",
        "grad_fn = jax.jit(jax.grad(lambda p, b: loss_fn(p, model, b)))\n",
        "jit_loss = jax.jit(lambda p, b: loss_fn(p, model, b))\n",
        "lr, wd = 3e-4, 0.01\n",
        "\n",
        "losses = []\n",
        "for step in tqdm(range(1, num_steps + 1), desc=\"Training\"):\n",
        "    batch = jnp.array(get_batch(data, batch_size, seq_len, np_rng))\n",
        "    grads = grad_fn(params, batch)\n",
        "    params, opt_state = adamw_step(params, grads, opt_state, lr=lr, weight_decay=wd)\n",
        "    if step % 10 == 0:\n",
        "        loss_val = float(jit_loss(params, batch))\n",
        "        losses.append((step, loss_val))\n",
        "\n",
        "save_checkpoint(str(DEMO_DIR / \"ckpt.pkl\"), params, opt_state, num_steps, cfg)\n",
        "print(\"Checkpoint saved.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "steps, vals = zip(*losses)\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(steps, vals, \"b-\", label=\"Loss\")\n",
        "plt.plot(steps, np.exp(vals), \"g-\", alpha=0.7, label=\"Perplexity\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss / Perplexity\")\n",
        "plt.legend()\n",
        "plt.title(\"Training loss (demo)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate text (standard KV cache)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "out = generate(model, params, tokenizer, \"The dog\", max_tokens=40, temperature=0.8, top_k=40, seed=123)\n",
        "print(\"Generated (KV cache):\")\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generate with MLA + DSA (sparse top-k)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mla_params = init_mla_params(jax.random.PRNGKey(0), cfg, d_latent=32)\n",
        "out_mla = generate(model, params, tokenizer, \"The cat\", max_tokens=40, temperature=0.8, top_k=40,\n",
        "                   seed=123, use_mla=True, mla_params=mla_params, sparse_top_k=16)\n",
        "print(\"Generated (MLA + DSA top-16):\")\n",
        "print(out_mla)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Tokenizer experiments (CS336 A1 §2.7)\n",
        "\n",
        "Compression ratio (bytes/token) and rough throughput. For full experiments use 10 sampled documents and report in writeup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sample_text = SAMPLE[: 5000]\n",
        "ids = tokenizer.encode(sample_text)\n",
        "bytes_per_token = len(sample_text.encode(\"utf-8\")) / max(1, len(ids))\n",
        "print(f\"Compression ratio (bytes/token): {bytes_per_token:.2f}\")\n",
        "import time\n",
        "t0 = time.perf_counter()\n",
        "for _ in range(10):\n",
        "    tokenizer.encode(sample_text)\n",
        "elapsed = time.perf_counter() - t0\n",
        "throughput_bps = (10 * len(sample_text.encode(\"utf-8\"))) / elapsed\n",
        "print(f\"Throughput (bytes/s): ~{throughput_bps:.0f}\")\n",
        "print(\"Perplexity (exp of mean loss) on last batch:\", float(np.exp(np.mean(vals))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CS336 Assignment 1 alignment\n",
        "\n",
        "This notebook runs: **BPE training**, **data loading**, **training loop**, **checkpointing**, **loss/perplexity curve**, **decoding** (temperature + top-k), and **tokenizer experiments** (compression ratio, throughput, perplexity). It does **not** implement the written problems (Unicode, accounting, ablations, etc.).\n",
        "\n",
        "- **Full checklist:** [docs/CS336_ASSIGNMENT1_CHECKLIST.md](../docs/CS336_ASSIGNMENT1_CHECKLIST.md) — maps every A1 problem to this codebase and to writeup deliverables.\n",
        "- **Written deliverables** (for writeup.pdf): §2.1 unicode1, §2.2 unicode2, §2.5 train_bpe_tinystories/owt, §2.7 tokenizer_experiments, §3 resource accounting, §4 learning_rate_tuning & adamwAccounting, §5 checkpointing, §7 learning_rate, batch_size_experiment, generate, ablations (layer_norm, pre_norm, no_pos_emb, swiglu), main_experiment (OWT)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}