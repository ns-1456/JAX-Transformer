{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TinyStories speed-run\n",
        "\n",
        "Minimal notebook: download TinyStories → train BPE → encode → train LM (fixed steps or full run) → plot loss → generate. Target: efficient run on A100/H100, validation loss ≤ 1.45. See `docs/SPEEDRUN_BENCHMARK.md`.\n",
        "\n",
        "**Colab-friendly:** Open this notebook in Google Colab (\"Open in Colab\" from GitHub). If you opened a blank Colab, set `COLAB_CLONE_URL` in the Setup cell to your repo and run it to clone + install. Enable a GPU: **Runtime → Change runtime type → T4 GPU** (or A100 if available)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "marker = Path(\"src\") / \"tokenizer.py\"\n",
        "\n",
        "def find_root():\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for _ in range(10):\n",
        "        if (cwd / marker).exists():\n",
        "            return cwd\n",
        "        if (cwd / \"project-2-lm-jax\" / marker).exists():\n",
        "            return cwd / \"project-2-lm-jax\"\n",
        "        if cwd == cwd.parent:\n",
        "            break\n",
        "        cwd = cwd.parent\n",
        "    return None\n",
        "\n",
        "if IN_COLAB:\n",
        "    ROOT = find_root()\n",
        "    if ROOT is None:\n",
        "        # Clone repo: set this to your repo URL if you opened a blank Colab\n",
        "        COLAB_CLONE_URL = \"https://github.com/ns-1456/JAX-XLA-LM.git\"\n",
        "        clone_dir = Path(\"/content\") / \"JAX-XLA-LM\"\n",
        "        if not (clone_dir / marker).exists():\n",
        "            get_ipython().system(f\"git clone --depth 1 {COLAB_CLONE_URL} {clone_dir}\")\n",
        "        ROOT = clone_dir\n",
        "    get_ipython().run_line_magic(\"pip\", \"install -q -r \" + str(ROOT / \"requirements.txt\"))\n",
        "    # Ensure we're in repo root for relative paths\n",
        "    os.chdir(ROOT)\n",
        "else:\n",
        "    ROOT = Path(os.environ.get(\"LM_PROJECT_ROOT\", \"\")).resolve() if os.environ.get(\"LM_PROJECT_ROOT\") else None\n",
        "    if not ROOT or not (ROOT / marker).exists():\n",
        "        ROOT = find_root()\n",
        "    if not ROOT or not (ROOT / marker).exists():\n",
        "        raise FileNotFoundError(\"Set LM_PROJECT_ROOT or run from project-2-lm-jax.\")\n",
        "\n",
        "sys.path.insert(0, str(ROOT / \"src\"))\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from tokenizer import train_bpe, Tokenizer\n",
        "from model import TransformerConfig, TransformerLM\n",
        "from optimizer import init_adamw, adamw_step\n",
        "from train import get_batch, loss_fn, save_checkpoint\n",
        "from generate import generate\n",
        "\n",
        "OUT_DIR = ROOT / \"notebooks\" / \"demo_outputs\"\n",
        "DATA_DIR = OUT_DIR / \"data\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "EOT = \"<|endoftext|>\"\n",
        "\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"Colab:\", IN_COLAB)\n",
        "devices = jax.devices()\n",
        "print(\"JAX devices:\", devices)\n",
        "if IN_COLAB and not devices:\n",
        "    print(\"No GPU detected. Runtime → Change runtime type → T4 GPU (or A100).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download TinyStories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# TinyStories (public); on Colab this caches under ~/.cache/huggingface\n",
        "for split in (\"train\", \"valid\"):\n",
        "    fn = f\"TinyStoriesV2-GPT4-{split}.txt\"\n",
        "    cached = hf_hub_download(repo_id=\"roneneldan/TinyStories\", filename=fn, repo_type=\"dataset\")\n",
        "    shutil.copy(cached, DATA_DIR / fn)\n",
        "path_ts = DATA_DIR / \"TinyStoriesV2-GPT4-train.txt\"\n",
        "print(\"TinyStories train:\", path_ts, \"| size:\", path_ts.stat().st_size // 1_000_000, \"MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train BPE and encode corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab, merges = train_bpe(str(path_ts), vocab_size=10000, special_tokens=[EOT], max_bytes=5_000_000, use_fast=True)\n",
        "tokenizer = Tokenizer(vocab=vocab, merges=merges, special_tokens=[EOT])\n",
        "tokenizer.save(str(OUT_DIR / \"tokenizer_tinystories.json\"))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "with open(path_ts) as f:\n",
        "    full_text = f.read()\n",
        "ids = np.array(tokenizer.encode(full_text), dtype=np.uint16)\n",
        "np.save(OUT_DIR / \"tinystories_ids.npy\", ids)\n",
        "data = np.load(OUT_DIR / \"tinystories_ids.npy\").astype(np.int32)\n",
        "print(f\"Tokenizer: vocab_size={vocab_size}. Data: {len(data):,} tokens.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train LM (speed-run or full)\n",
        "\n",
        "Set `NUM_STEPS` for a fixed-step speed run, or a large value (e.g. 100_000+) for full pretraining. **Colab:** T4 free tier — use `BATCH_SIZE = 32` to avoid OOM; A100 (Colab Pro) — 64–128 is fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_STEPS = 30_000   # speed-run; use 100_000+ for full pretraining\n",
        "BATCH_SIZE = 32      # 32 for Colab T4; 64–128 for A100/H100\n",
        "SEQ_LEN = 256\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "LOG_EVERY = 200\n",
        "SAVE_EVERY = 5000\n",
        "SEED = 42\n",
        "\n",
        "cfg = TransformerConfig(vocab_size=vocab_size, d_model=256, num_layers=6, num_heads=8, d_ff=1024, max_seq_len=SEQ_LEN, dropout_rate=0.1)\n",
        "model = TransformerLM(cfg)\n",
        "rng = jax.random.PRNGKey(SEED)\n",
        "params = model.init(rng, jnp.ones((1, SEQ_LEN), dtype=jnp.int32), deterministic=True)\n",
        "opt_state = init_adamw(params)\n",
        "np_rng = np.random.default_rng(SEED)\n",
        "grad_fn = jax.jit(jax.grad(lambda p, b: loss_fn(p, model, b)))\n",
        "jit_loss = jax.jit(lambda p, b: loss_fn(p, model, b))\n",
        "\n",
        "nparams = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
        "print(f\"Model: {cfg.num_layers}L-{cfg.d_model}D, {nparams:,} params. Steps: {NUM_STEPS}, batch={BATCH_SIZE}, seq_len={SEQ_LEN}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "losses = []\n",
        "t0 = time.time()\n",
        "for step in tqdm(range(1, NUM_STEPS + 1), desc=\"Training\"):\n",
        "    batch = jnp.array(get_batch(data, BATCH_SIZE, SEQ_LEN, np_rng))\n",
        "    grads = grad_fn(params, batch)\n",
        "    params, opt_state = adamw_step(params, grads, opt_state, lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_val = float(jit_loss(params, batch))\n",
        "        losses.append((step, loss_val))\n",
        "        elapsed = time.time() - t0\n",
        "        tok_per_sec = (step * BATCH_SIZE * SEQ_LEN) / elapsed\n",
        "        tqdm.write(f\"step {step} | loss {loss_val:.4f} | ppl {np.exp(loss_val):.2f} | {tok_per_sec:.0f} tok/s\")\n",
        "    if step % SAVE_EVERY == 0:\n",
        "        save_checkpoint(str(OUT_DIR / f\"ckpt_step_{step}.pkl\"), params, opt_state, step, cfg)\n",
        "\n",
        "elapsed = time.time() - t0\n",
        "save_checkpoint(str(OUT_DIR / \"ckpt_final.pkl\"), params, opt_state, NUM_STEPS, cfg)\n",
        "print(f\"Done in {elapsed/60:.1f} min. Final checkpoint: ckpt_final.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if losses:\n",
        "    steps, vals = zip(*losses)\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(steps, vals, \"b-\", label=\"Loss\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"TinyStories speed-run\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(f\"Final logged loss: {vals[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = generate(model, params, tokenizer, \"Once upon a time\", max_tokens=256, temperature=0.8, top_k=40, seed=99)\n",
        "print(out)\n",
        "print(f\"\\n~{len(tokenizer.encode(out))} tokens\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
